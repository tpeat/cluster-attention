{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel, BertConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' # or none or cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 30522\n",
      "[101, 2879, 2079, 1045, 2293, 17953, 2361, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] boy do i love nlp [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of vocab:\", tokenizer.vocab_size)\n",
    "tokens = tokenizer.encode(\"Boy do i love NLP\")\n",
    "print(tokens)\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_text = self.dataset[idx][\"translation\"][\"en\"]\n",
    "        target_text = self.dataset[idx][\"translation\"][\"fr\"]\n",
    "\n",
    "        source_encodings = self.tokenizer(source_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_encodings['input_ids'].flatten(),\n",
    "            'attention_mask': source_encodings['attention_mask'].flatten(),\n",
    "            'labels': target_encodings['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_data = books['train']\n",
    "valid_data = books['test']\n",
    "\n",
    "train_dataset = TranslationDataset(train_data, tokenizer)\n",
    "valid_dataset = TranslationDataset(valid_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has these keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Each value is a tensor of shape batchxd: torch.Size([16, 128])\n",
      "An example input, label pair:\n",
      "English: [CLS] mais continuez donc, qu ’ avez - vous fait apres cela? » [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "French: [CLS] go on, though. what did you do next? \" [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(\"Each batch has these keys:\", batch.keys())\n",
    "print(\"Each value is a tensor of shape batchxd:\", batch['input_ids'].shape)\n",
    "print(\"An example input, label pair:\")\n",
    "print(\"English:\", tokenizer.decode(batch['input_ids'][0]))\n",
    "print(\"French:\", tokenizer.decode(batch['labels'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "# Initialize the encoder and decoder as a single model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "\n",
    "# Configurations for the model (Optional but useful for fine-tuning)\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "# Freeze BERT encoder layers to fine-tune only the decoder\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./translation_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Example translation\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Translate an example sentence\n",
    "print(translate(\"This is a test sentence for translation.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving beyond Huggingface\n",
    "\n",
    "We need more control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, max_length=128):\n",
    "        super(TransformerMT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_length, d_model))\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_emb = self.embedding(src) + self.pos_encoder[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.pos_encoder[:, :tgt.size(1), :]\n",
    "        \n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = TransformerMT(vocab_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_encoder torch.Size([1, 128, 512])\n",
      "embedding.weight torch.Size([30522, 512])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.0.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.0.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.0.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.0.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.0.norm2.bias torch.Size([512])\n",
      "transformer.encoder.layers.1.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.1.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.1.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.1.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.1.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.1.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.1.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.1.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.1.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.1.norm2.bias torch.Size([512])\n",
      "transformer.encoder.layers.2.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.2.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.2.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.2.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.2.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.2.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.2.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.2.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.2.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.2.norm2.bias torch.Size([512])\n",
      "transformer.encoder.layers.3.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.3.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.3.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.3.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.3.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.3.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.3.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.3.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.3.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.3.norm2.bias torch.Size([512])\n",
      "transformer.encoder.layers.4.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.4.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.4.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.4.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.4.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.4.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.4.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.4.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.4.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.4.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.4.norm2.bias torch.Size([512])\n",
      "transformer.encoder.layers.5.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.encoder.layers.5.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.encoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.encoder.layers.5.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.encoder.layers.5.linear1.weight torch.Size([2048, 512])\n",
      "transformer.encoder.layers.5.linear1.bias torch.Size([2048])\n",
      "transformer.encoder.layers.5.linear2.weight torch.Size([512, 2048])\n",
      "transformer.encoder.layers.5.linear2.bias torch.Size([512])\n",
      "transformer.encoder.layers.5.norm1.weight torch.Size([512])\n",
      "transformer.encoder.layers.5.norm1.bias torch.Size([512])\n",
      "transformer.encoder.layers.5.norm2.weight torch.Size([512])\n",
      "transformer.encoder.layers.5.norm2.bias torch.Size([512])\n",
      "transformer.encoder.norm.weight torch.Size([512])\n",
      "transformer.encoder.norm.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.0.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.0.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.0.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.0.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.0.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.0.norm3.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.1.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.1.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.1.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.1.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.1.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.1.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.1.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.1.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.1.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.1.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.1.norm3.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.2.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.2.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.2.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.2.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.2.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.2.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.2.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.2.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.2.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.2.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.2.norm3.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.3.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.3.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.3.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.3.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.3.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.3.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.3.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.3.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.3.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.3.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.3.norm3.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.4.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.4.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.4.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.4.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.4.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.4.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.4.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.4.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.4.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.4.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.4.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.4.norm3.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.5.self_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.5.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.multihead_attn.in_proj_weight torch.Size([1536, 512])\n",
      "transformer.decoder.layers.5.multihead_attn.in_proj_bias torch.Size([1536])\n",
      "transformer.decoder.layers.5.multihead_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.decoder.layers.5.multihead_attn.out_proj.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.linear1.weight torch.Size([2048, 512])\n",
      "transformer.decoder.layers.5.linear1.bias torch.Size([2048])\n",
      "transformer.decoder.layers.5.linear2.weight torch.Size([512, 2048])\n",
      "transformer.decoder.layers.5.linear2.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.norm1.weight torch.Size([512])\n",
      "transformer.decoder.layers.5.norm1.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.norm2.weight torch.Size([512])\n",
      "transformer.decoder.layers.5.norm2.bias torch.Size([512])\n",
      "transformer.decoder.layers.5.norm3.weight torch.Size([512])\n",
      "transformer.decoder.layers.5.norm3.bias torch.Size([512])\n",
      "transformer.decoder.norm.weight torch.Size([512])\n",
      "transformer.decoder.norm.bias torch.Size([512])\n",
      "fc_out.weight torch.Size([30522, 512])\n",
      "fc_out.bias torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0], param[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
